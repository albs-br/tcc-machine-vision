{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_PATH = \"../input/animals-data/dataset/\"\n",
    "IMG_PATH = \"C:/TCC_ForaDoOneDrive/CNRPark-Patches-150x150/A/Imagens Selecionadas 512x512/\"\n",
    "IMG_HEIGHT = 512  # The images are already resized here\n",
    "IMG_WIDTH = 512  # The images are already resized here\n",
    "\n",
    "# redimensiona a imagem\n",
    "# img = cv2.resize(\n",
    "# img, (160, 160), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_RATIO = 0.75\n",
    "VAL_RATIO = 1 - TRAIN_RATIO\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 2\n",
    "TRAIN_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n",
    "TEST_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n",
    "FULL_BATCH_SIZE = 32\n",
    "\n",
    "AUTOENCODER_MODEL_PATH = \"baseline_autoencoder.pt\"\n",
    "\n",
    "# ENCODER_MODEL_PATH = \"deep_encoder.pt\"\n",
    "# DECODER_MODEL_PATH = \"deep_decoder.pt\"\n",
    "\n",
    "ENCODER_MODEL_PATH = \"baseline_encoder.pt\"\n",
    "DECODER_MODEL_PATH = \"baseline_decoder.pt\"\n",
    "\n",
    "EMBEDDING_SHAPE = (1, 64, 64, 64)\n",
    "# TEST_RATIO = 0.2\n",
    "\n",
    "APP_PATH = \"C:/Users/albs_/source/repos/tcc-machine-vision/image_similarity-master/image_similarity/\"\n",
    "EMBEDDING_PATH = APP_PATH + \"data_embedding_f.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER_MODEL_PATH: baseline_encoder.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ConvEncoder:\n\tMissing key(s) in state_dict: \"conv4.weight\", \"conv4.bias\", \"conv5.weight\", \"conv5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-911fd0c0a96e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Load the state dict of encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mENCODER_MODEL_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ConvEncoder:\n\tMissing key(s) in state_dict: \"conv4.weight\", \"conv4.bias\", \"conv5.weight\", \"conv5.bias\". "
     ]
    }
   ],
   "source": [
    "#from flask import Flask, request, json\n",
    "import torch_model\n",
    "#import config\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load the model before we start the server\n",
    "encoder = torch_model.ConvEncoder()\n",
    "\n",
    "print('ENCODER_MODEL_PATH: ' + ENCODER_MODEL_PATH)\n",
    "\n",
    "# Load the state dict of encoder\n",
    "encoder.load_state_dict(torch.load(ENCODER_MODEL_PATH, map_location=device))\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "\n",
    "# Loads the embedding\n",
    "embedding = np.load(EMBEDDING_PATH)\n",
    "\n",
    "print(\"Loaded model and embeddings\")\n",
    "\n",
    "\n",
    "def compute_similar_images(image_tensor, num_images, embedding, device):\n",
    "    \"\"\"\n",
    "    Given an image and number of similar images to generate.\n",
    "    Returns the num_images closest neares images.\n",
    "\n",
    "    Args:\n",
    "    image_tenosr: PIL read image_tensor whose similar images are needed.\n",
    "    num_images: Number of similar images to find.\n",
    "    embedding : A (num_images, embedding_dim) Embedding of images learnt from auto-encoder.\n",
    "    device : \"cuda\" or \"cpu\" device.\n",
    "    \"\"\"\n",
    "\n",
    "    print('compute_similar_images')\n",
    "    \n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = encoder(image_tensor).cpu().detach().numpy()\n",
    "\n",
    "    print(image_embedding.shape)\n",
    "\n",
    "    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n",
    "    print(flattened_embedding.shape)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(embedding)\n",
    "\n",
    "    _, indices = knn.kneighbors(flattened_embedding)\n",
    "    indices_list = indices.tolist()\n",
    "    print(indices_list)\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "def compute_similar_features(image, num_images, embedding, nfeatures=30):\n",
    "    \"\"\"\n",
    "    Given a image, it computes features using ORB detector and finds similar images to it\n",
    "    Args:\n",
    "    image: Opencv read Image whose features and simlar images are required.\n",
    "    num_images: Number of similar images required.\n",
    "    embedding: 2 Dimensional Embedding vector.\n",
    "    nfeatures: (optional) Number of features ORB needs to compute\n",
    "    \"\"\"\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=nfeatures)\n",
    "\n",
    "    # Detect features\n",
    "    keypoint_features = orb.detect(image)\n",
    "    # compute the descriptors with ORB\n",
    "    keypoint_features, des = orb.compute(image, keypoint_features)\n",
    "\n",
    "    # des contains the description to features\n",
    "\n",
    "    des = des / 255.0\n",
    "    des = np.expand_dims(des, axis=0)\n",
    "    des = np.reshape(des, (des.shape[0], -1))\n",
    "    # print(des.shape)\n",
    "    # print(embedding.shape)\n",
    "\n",
    "    pca = PCA(n_components=des.shape[-1])\n",
    "    reduced_embedding = pca.fit_transform(\n",
    "        embedding,\n",
    "    )\n",
    "    # print(reduced_embedding.shape)\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(reduced_embedding)\n",
    "    _, indices = knn.kneighbors(des)\n",
    "\n",
    "    indices_list = indices.tolist()\n",
    "    # print(indices_list)\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @app.route(\"/simfeat\", methods=[\"POST\"])\n",
    "# def simfeat():\n",
    "#     r = request.files[\"image\"]\n",
    "#     # print(\"Hi\")\n",
    "#     # convert string of image data to uint8\n",
    "#     nparr = np.fromstring(r.data, np.uint8)\n",
    "#     # decode image\n",
    "#     img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "#     indices_list = compute_similar_features(img, num_images=5, embedding=embedding)\n",
    "#     # Need to display the images\n",
    "#     return (\n",
    "#         json.dumps({\"indices_list\": indices_list}),\n",
    "#         200,\n",
    "#         {\"ContentType\": \"application/json\"},\n",
    "#     )\n",
    "\n",
    "#print('config.APP_PATH: ' + config.APP_PATH)\n",
    "\n",
    "image = Image.open(\"C:/TCC_ForaDoOneDrive\\CNRPark-Patches-150x150/A/Search/busy 20150703_0920_14 512x512.jpg\")\n",
    "print(image)\n",
    "image_tensor = T.ToTensor()(image)\n",
    "#print(image_tensor)\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "#print(image_tensor)\n",
    "indices_list = compute_similar_images(\n",
    "    image_tensor, num_images=5, embedding=embedding, device=device\n",
    ")\n",
    "print(indices_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
